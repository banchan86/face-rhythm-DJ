{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\n",
    "<center><h1>Face Rhythm</h1></center>\n",
    "\n",
    "***\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"https://images.squarespace-cdn.com/content/5688a31305f8e23aa2893502/1614723283221-5Z5038AT7Y6KCOM2PIU4/Screenshot+from+2021-03-02+17-05-12.png?content-type=image%2Fpng\" style=\"height: 200px\"> </td>\n",
    "<td> <img src=\"https://images.squarespace-cdn.com/content/5688a31305f8e23aa2893502/1614723856628-J89PYYSF7K7JATE2KMF9/Screenshot+from+2021-03-02+17-23-46.png?format=300w&content-type=image%2Fpng\" style=\"height: 200px\"> </td>\n",
    "<td> <img src=\"https://images.squarespace-cdn.com/content/5688a31305f8e23aa2893502/1614723931026-OORV0RAPZNWV3R8TBOXB/Screenshot+from+2021-03-02+17-25-11.png?format=300w&content-type=image%2Fpng\" style=\"height: 200px\"> </td>\n",
    "<td> <img src=\"https://images.squarespace-cdn.com/content/5688a31305f8e23aa2893502/1614724055033-O3GBEF1D9MULFZKI2IUJ/Screenshot+from+2021-03-02+17-27-10.png?format=300w&content-type=image%2Fpng\" style=\"height: 200px\"> </td>\n",
    "<td> <img src=\"https://images.squarespace-cdn.com/content/5688a31305f8e23aa2893502/1614723378405-WXN74ZTT1KYZUQGDM07X/face_rhythm_banner2.png?format=1000w&content-type=image%2Fpng\" style=\"height: 200px\"> </td>\n",
    "</tr></table>\n",
    "\n",
    "***\n",
    "\n",
    "##### Notebook Shortcuts\n",
    "- **[Notebook Setup](#Notebook-Setup)**: Prepare all the necessary config files and folders\n",
    "- **[Set ROI](#Set-ROI)**: Set the ROI for the analysis\n",
    "- **[Run Optic Flow](#Run-Optic-Flow)**: Run the optic flow analysis\n",
    "- **[Clean Optic Flow](#Clean-Optic-Flow)**: Optic flow post-processing\n",
    "- **[Convolutional Dimensionality Reduction](#Convolutional-Dimensionality-Reduction)**: Convolutional Dimensionality Reduction\n",
    "- **[Analysis](#Analysis)**: Decompose and Analyze the optic flow data in many ways\n",
    "- **[Comparisons](#Comparisons)**: Compare Face Rhythm to some peer algorithms\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Tips on running this notebook:\n",
    "In theory it would be nice if you could just enter the path of the video(s) and just let it run all the way through. In practice, there are a few hoops to jump through\n",
    "- Run the Notebook Setup Block (two blocks below this one). This should pretty much always be done, even if you are loading precomputed file from disk instead of calculating them. This step loads in some useful meta data used throughout.\n",
    "- Even if you are restarting at a specific point in your analysis, run your Setup Block then head down to your current analysis step cell "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Directory Organization\n",
    "------------\n",
    "\n",
    "    Project Directory\n",
    "    ├── config.yaml           <- Configuration parameters to run each module in the pipeline. Dictionary.\n",
    "    ├── run_info.json         <- Output information from each module. Dictionary.\n",
    "    │\n",
    "    ├── run_data              <- Output data from each module.\n",
    "    │   ├── Dataset_videos.h5 <- Output data from Dataset_videos class. Contains metadata about the videos.\n",
    "    │   ├── ROIs.h5           <- Output data from ROIs class. Contains ROI masks.\n",
    "    │   ├── PointTracker.h5   <- Output data from PointTracker class. Contains point tracking data.\n",
    "    |   ├── VQT_Analyzer.h5   <- Output data from VQT_Analyzer class. Contains spectral decomposition data.\n",
    "    │   ├── TCA.h5            <- Output data from TCA class. Contains TCA decomposition data.\n",
    "    │   \n",
    "    └── visualizations        <- Output visualizations.\n",
    "        ├── factors_rearranged_[frequency].png  <- Example of a rearranged factor plot.\n",
    "        └── point_tracking_demo.avi             <- Example video.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\n",
    "<center><h1>Notebook Setup</h1></center>\n",
    "\n",
    "***\n",
    "\n",
    "### Creates config and locates videos\n",
    "\n",
    "**Crucially, always run this first cell every time you run this notebook.**\n",
    "\n",
    "Also, generally make sure to read through the config parameters before running.\n",
    "\n",
    "The Project path is the path to a folder (existing or not) where we will store our derived files. I recommend creating a project folder and then copying this notebook into that folder.\n",
    "The Video path is the path to a folder containing videos. \n",
    "The run name will determine the name of the config. You might create multiple configs if you want to re-run the same data with slightly different parameters\n",
    "\n",
    "Previous face rhythm users might be familiar with the 'sessions' structure. Some users will want to run multiple sessions through Face Rhythm at the same time. If that's you, then read the docs to see what parameters to change:\n",
    "https://face-rhythm.readthedocs.io/\n",
    "\n",
    "If you did everything according to the readme, you should see that the video_path currently points to a folder containing one sample video in the testing folder. Give this a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ALWAYS RUN THIS CELL\n",
    "# widen jupyter notebook window\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:95% !important; }</style>\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import face_rhythm as fr\n",
    "\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot\n",
    "\n",
    "fr.util.system_info(verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these to your desired project path and video path\n",
    "# Assuming that your notebook is running  in your project directory, \n",
    "# then the project directory is current working directory\n",
    "# Also, we have some test videos in the face_rhythm repo, so you can set the video directory there for testing\n",
    "directory_project = '/Users/richardhakim/Downloads/test'\n",
    "directory_videos  = '/Users/richardhakim/Downloads'\n",
    "\n",
    "filename_strMatch = 'video2025-02-20T11_29_56-00.00.00.000-00.00.08.520'  ## You can use regular expressions to search and match more complex strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_config, path_run_info, directory_project = fr.project.prepare_project(\n",
    "    directory_project=directory_project,\n",
    "    overwrite_config=False,  ## WARNING! CHECK THIS.\n",
    "    mkdir=True,    \n",
    "    initialize_visualization=True,    \n",
    "    verbose=2,\n",
    ")\n",
    "figure_saver = fr.util.Figure_Saver(\n",
    "    path_config=path_config,\n",
    "    formats_save=['png'],\n",
    "    kwargs_savefig={'bbox_inches': 'tight', 'pad_inches': 0.1, 'transparent': True, 'dpi': 300},\n",
    "    overwrite=True,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare video data for point tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_videos = fr.helpers.find_paths(\n",
    "    dir_outer=directory_videos,\n",
    "    reMatch=filename_strMatch,  ## string to use to search for files in directory. Uses regular expressions!\n",
    "    depth=0,  ## how many folders deep to search\n",
    ")\n",
    "\n",
    "pprint(paths_videos, width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a `BufferedVideoReader` object for reading video file data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = fr.helpers.BufferedVideoReader(\n",
    "#     video_readers=data.videos, \n",
    "    paths_videos=paths_videos,\n",
    "    buffer_size=1000, \n",
    "    prefetch=1, \n",
    "    posthold=1,\n",
    "    method_getitem='by_video',\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a `Dataset_videos` object for referencing the raw video data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fr.data_importing.Dataset_videos(\n",
    "    bufferedVideoReader=videos,\n",
    "#     paths_videos=paths_videos,\n",
    "    contiguous=False,\n",
    "#     frame_rate_clamp=240,\n",
    "    verbose=2,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the `Dataset_videos` object in the 'analysis_files' project folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.save_config(path_config=path_config, overwrite=True, verbose=1)\n",
    "data.save_run_info(path_config=path_config, overwrite=True, verbose=1)\n",
    "data.save_run_data(path_config=path_config, overwrite=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define ROIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either select new ROIs (`select_mode='gui'`) and provide an exampleImage, or import existing ROIs (`path_file=path_to_ROIs.h5_file`).\\\n",
    "**Typically, you should make 1 or 2 ROIs. The first defining where the face points should be and the second for cropping the frame.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exampleImage = data[0][0]\n",
    "# %matplotlib notebook\n",
    "rois = fr.rois.ROIs(\n",
    "    select_mode='gui',\n",
    "    exampleImage=exampleImage,\n",
    "#     select_mode='file',\n",
    "#     path_file=str(Path('/home/rich/Desktop/0322N_and_0322R/mouse_0322N__20230430/') / 'ROIs.h5'),\n",
    "#     path_file=r'/home/rich/Desktop/ROIs_7.h5',\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a point grid out of the first ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois.make_points(rois=rois[0], point_spacing=13) if rois.point_positions is None else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the `ROIs` object in the 'analysis_files' project folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois.save_config(path_config=path_config, overwrite=True, verbose=1)\n",
    "rois.save_run_info(path_config=path_config, overwrite=True, verbose=1)\n",
    "rois.save_run_data(path_config=path_config, overwrite=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize the ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois.plot_rois(exampleImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare `PointTracker` object.\\\n",
    "Set `visualize_video` to **`True`** to tune parameters until they look appropriate, then set to **`False`** to run the full dataset through at a much faster speed.\n",
    "\n",
    "Key parameters:\n",
    "- `point_spacing`: distance between points. Vary so that total number of points is appropriate.\n",
    "- `mesh_rigidity`: how rigid the mesh elasticity is. Vary so that points track well without drift.\n",
    "- `relaxation`: how quickly the points relax back to their home position. Vary so that points track well without dift.\n",
    "- `kwargs_method > winSize`: the spatial size of the optical flow calculation. Smaller is better but noisier, larger is less accurate but more robust to noise.\n",
    "- `params_outlier_handling > threshold_displacement`: point displacements above this value will result in freezing of the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = fr.point_tracking.PointTracker(\n",
    "    visualize_video=False,\n",
    "#     buffered_video_reader=videos[:5],\n",
    "    buffered_video_reader=videos,\n",
    "    point_positions=rois.point_positions,\n",
    "    rois_masks=[rois[1]],\n",
    "    contiguous=True,\n",
    "    params_optical_flow={\n",
    "        \"method\": \"lucas_kanade\",\n",
    "        \"mesh_rigidity\": 0.02,\n",
    "        \"mesh_n_neighbors\": 70,\n",
    "        \"relaxation\": 0.003,\n",
    "        \"kwargs_method\": {\n",
    "            \"winSize\": [60, 60],\n",
    "            \"maxLevel\": 5,\n",
    "            \"criteria\": [cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 20, 0.0003],\n",
    "        },        \n",
    "    },\n",
    "    params_clahe={\n",
    "        \"clipLimit\": 60.0,\n",
    "        \"tileGridSize\": (250, 250),\n",
    "    },\n",
    "    params_visualization={\n",
    "        'alpha': 0.9,\n",
    "        'point_sizes': 3,\n",
    "    },\n",
    "    params_outlier_handling = {\n",
    "        'threshold_displacement': 120,  ## Maximum displacement between frames, in pixels.\n",
    "        'framesHalted_before': 10,  ## Number of frames to halt tracking before a violation.\n",
    "        'framesHalted_after': 10,  ## Number of frames to halt tracking after a violation.\n",
    "    },\n",
    "    idx_start=0,  ## generally only change for visualization.\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform point tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.track_points()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the `PointTracker` object in 'analysis_files' project directory.\\\n",
    "Using compression can reduce file sizes slightly but is very slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.save_config(path_config=path_config, overwrite=True, verbose=1)\n",
    "pt.save_run_info(path_config=path_config, overwrite=True, verbose=2)\n",
    "pt.save_run_data(path_config=path_config, overwrite=True, use_compression=False, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear some memory if needed. Optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the `PointTracker` data as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_data = fr.h5_handling.simple_load(str(Path(directory_project) / 'analysis_files' / 'PointTracker.h5'), return_dict=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare `VQT_Analyzer` object.\n",
    "\n",
    "Key parameters:\n",
    "- `Q_lowF`:  Quality of the lowest frequency band of the spectrogram. Q value is number of oscillation periods.\n",
    "- `Q_highF`: Quality of the highest frequency band...\n",
    "- `F_min`: Lowest frequency band to use.\n",
    "- `F_max`: Highest frequency band to use.\n",
    "- `downsample_factor`: How much to downsample the spectrogram by in time.\n",
    "- `return_complex`: Whether or not to return the complex spectrogram. Generally set to False unless you want to try something fancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "Fs = fr.util.load_run_info_file(path_run_info)['Dataset_videos']['frame_rate']\n",
    "\n",
    "spec = fr.spectral_analysis.VQT_Analyzer(\n",
    "    params_VQT = {\n",
    "        'Fs_sample': Fs, \n",
    "        'Q_lowF': 2, \n",
    "        'Q_highF': 5, \n",
    "        'F_min': 1.0, \n",
    "        'F_max': data.frame_rate//2, \n",
    "        'n_freq_bins': 25, \n",
    "        'win_size': 301, \n",
    "        'plot_pref': True, \n",
    "        'downsample_factor': 8, \n",
    "        'padding': 'valid',\n",
    "        'DEVICE_compute': fr.helpers.set_device(use_GPU=True), \n",
    "        'batch_size': 10,\n",
    "        'return_complex': False, \n",
    "        'progressBar': True,\n",
    "    },\n",
    "    normalization_factor=0.95,\n",
    "    spectrogram_exponent=1.0,\n",
    "    one_over_f_exponent=0.5,\n",
    "    verbose = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at a demo spectrogram of a single point.\\\n",
    "Specify the point with the `idx_point` and `name_points` fields.\\\n",
    "Note that the `pt_data['points_tracked']` dictionary holds subdictionaries withe numeric string names (ie `['0'], ['1']`) for each video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "spec_demo, xAxis_demo, freqs_demo = spec.demo_transform(\n",
    "    points_tracked=pt_data['points_tracked'],\n",
    "    point_positions=pt_data['point_positions'],\n",
    "    idx_point=0,\n",
    "    name_points='0',\n",
    "    plot=True,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spec.transform_all(\n",
    "    points_tracked=pt_data['points_tracked'],\n",
    "    point_positions=pt_data['point_positions'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the `VQT_Analyzer` object in 'analysis_files' project directory.\\\n",
    "Using compression can reduce file sizes slightly but is very slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec.save_config(path_config=path_config, overwrite=True, verbose=1)\n",
    "spec.save_run_info(path_config=path_config, overwrite=True, verbose=1)\n",
    "spec.save_run_data(path_config=path_config, overwrite=True, use_compression=False, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear some memory if needed. Optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the `VQT_Analyzer` data as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_data = fr.h5_handling.simple_load(str(Path(directory_project) / 'analysis_files' / 'VQT_Analyzer.h5'), return_dict=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare `TCA` object, and then rearrange the data with the `.rearrange_data` method.\n",
    "\n",
    "Key parameters for `.rearrange_data`:\n",
    "- `names_dims_array`:  Enter the names of the dimensions of the spectrogram. Typically these are `'xy', 'points', 'frequency', 'time'`.\n",
    "- `names_dims_concat_array`: Enter any dimensions you wish to concatenate along other dimensions. Typically we wish to concatenate the `'xy'` dimension along the `'points'` dimension, so we make a list containing that pair as a tuple: `[('xy', 'points')]`.\n",
    "- `concat_complexDim`: If your input data are complex valued, then this can concatenate the complex dimension along another dimension.\n",
    "- `name_dim_dictElements`: The `data` argument is expected to be a dictionary of dictionaries of arrays, where the inner dicts are trials or videos. This is the name of what those inner dicts are. Typically `'trials'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectrograms = spec_data['spectrograms']\n",
    "spectrograms = {key: np.abs(val) for key,val in list(spec_data['spectrograms'].items())[:]}\n",
    "\n",
    "tca = fr.decomposition.TCA(verbose=2)\n",
    "\n",
    "tca.rearrange_data(\n",
    "    data=spectrograms,\n",
    "    names_dims_array = ['xy', 'points', 'frequency', 'time'],\n",
    "    names_dims_concat_array = [('xy', 'points')],\n",
    "    concat_complexDim=False,\n",
    "    name_dim_concat_complexDim='time',\n",
    "    name_dim_dictElements = 'session',\n",
    "    method_handling_dictElements = 'separate',\n",
    "    name_dim_concat_dictElements = 'time',\n",
    "    idx_windows=None,\n",
    "    name_dim_array_window='time',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, you can normalize along a specified axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tca.normalize_data(\n",
    "    mean_subtract=False,\n",
    "    std_divide=True, \n",
    "    dim_name='time', \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit TCA model.\n",
    "\n",
    "There are a few methods that can be used:\n",
    "- `'CP_NN_HALS'`: non-negative CP decomposition using the efficient HALS algorithm. This should be used in most cases.\n",
    "- `'CP'`: Standard CP decomposition. Use if input data are not non-negative (if you are using complex valued spectrograms or similar).\n",
    "- `'Randomized_CP'`: Randomized CP decomposition. Allows for large input tensors. If you are using huge tensors and you are memory constrained or want to run on a small GPU, this is your only option.\n",
    "\n",
    "If you have and want to use a CUDA compatible GPU:\n",
    "- Set `DEVICE` to `'cuda'`\n",
    "- GPU memory can be saved by setting `'init'` method to `'random'`. However, fastest convergence and highest accuracy typically come from `'init': 'svd'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tca.fit(\n",
    "    method='CP_NN_HALS',\n",
    "#     method='CP',\n",
    "    params_method={\n",
    "        'rank': 10, \n",
    "        'n_iter_max': 1000, \n",
    "        'init': 'random', \n",
    "        'svd': 'truncated_svd', \n",
    "        'tol': 1e-09, \n",
    "#         'nn_modes': [0,1], \n",
    "        'verbose': True, \n",
    "    },\n",
    "    DEVICE=fr.helpers.set_device(use_GPU=True),\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rearrange the factors.\\\n",
    "You can undo the concatenation that was done during `.rearrange_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tca.rearrange_factors(\n",
    "    undo_concat_complexDim=False,\n",
    "    undo_concat_dictElements=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the `TCA` object in 'analysis_files' project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tca.save_config(path_config=path_config, overwrite=True, verbose=1)\n",
    "tca.save_run_info(path_config=path_config, overwrite=True, verbose=1)\n",
    "tca.save_run_data(path_config=path_config, overwrite=True, use_compression=False, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear some memory if needed. Useful if you ran the fit on a GPU. Optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tca._cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "tca.plot_factors(\n",
    "    figure_saver=None,\n",
    "    show_figures=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the `PointTracker`, `TCA`, and `VQT` data as dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tca_data  = fr.h5_handling.simple_load(str(Path(directory_project) / 'analysis_files' / 'TCA.h5'), return_dict=True)\n",
    "spec_data = fr.h5_handling.simple_load(str(Path(directory_project) / 'analysis_files' / 'VQT_Analyzer.h5'), return_dict=True)\n",
    "pt_data   = fr.h5_handling.simple_load(str(Path(directory_project) / 'analysis_files' / 'PointTracker.h5'), return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize the spectral factors and corresponding spatial factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_tca_specFactor = plt.figure()\n",
    "plt.plot(spec_data['frequencies'], tca_data['factors']['0']['frequency'])\n",
    "plt.xscale('log')\n",
    "plt.xlabel('frequency')\n",
    "plt.legend(np.arange(tca_data['factors']['0']['frequency'].shape[1]) + 1);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_saver.save_figure(\n",
    "    fig=fig_tca_specFactor,\n",
    "    name_save=f'fig_tca_specFactor',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize the point factor magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = fr.helpers.simple_cmap(\n",
    "    colors=[[0,0,0], [1,0,0], [1,1,0]], \n",
    "    over=[0,1,0], \n",
    "    under=[0,1,0], \n",
    "    bad=[0,1,0], \n",
    ")\n",
    "\n",
    "factor_values = tca_data['factors_rearranged']['0']['(xy points)']\n",
    "factor_magnitudes = np.linalg.norm(factor_values.reshape(2, factor_values.shape[0]//2, -1), axis=0)\n",
    "\n",
    "pts_tmp = pt_data['point_positions']\n",
    "im_tmp = videos.video_readers[0][0].cpu().numpy()\n",
    "\n",
    "frame_visualizer = fr.visualization.FrameVisualizer(\n",
    "    verbose=2,\n",
    "    frame_height_width=videos.frame_height_width,\n",
    "    point_sizes=3,\n",
    "    alpha=0.6,\n",
    ")\n",
    "\n",
    "ims_factors_points = [frame_visualizer.visualize_image_with_points(image=im_tmp, points=pts_tmp, points_colors=[(cmap(f/f.max())*255).astype(np.int64)[:,:3]]) for f in factor_magnitudes.T] \n",
    "\n",
    "fr.visualization.display_toggle_image_stack(ims_factors_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare an image saving helper class. It can save individual images in multiple formats as well as a list of images as a GIF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_saver = fr.util.Image_Saver(\n",
    "    path_config=path_config,\n",
    "    dir_save=None,\n",
    "    overwrite=True,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save individual factor images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[image_saver.save_image(\n",
    "    array_image=im,\n",
    "    name_save=f'im_factors_points_{ii+1}',\n",
    "    formats_save=['png'],\n",
    ") for ii, im in enumerate(ims_factors_points)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save gif of all the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_saver.save_gif(\n",
    "    array_images=fr.helpers.add_text_to_images(\n",
    "        images=ims_factors_points,\n",
    "        text=[[f'factor: {ii+1}'] for ii in range(len(ims_factors_points))],\n",
    "        position=(10, 40),\n",
    "        font_size=1,\n",
    "        line_width=2,\n",
    "        color=(255,255,255),\n",
    "    ),\n",
    "    name_save=f'im_factors_points',\n",
    "    frame_rate=2.0,\n",
    "    loop=0,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo playback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playback a video with points overlayed.\\\n",
    "Make sure you have a `BufferedVideoReader` object called `videos` made of your videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_video_to_use = 0\n",
    "idx_frames_to_use = np.arange(0, 1000)\n",
    "\n",
    "videos.method_getitem = 'by_video'\n",
    "\n",
    "frame_visualizer = fr.visualization.FrameVisualizer(\n",
    "    display=True,\n",
    "    error_checking=True,\n",
    "    verbose=2,\n",
    "    path_save=str(Path(directory_project) / 'visualizations' / 'point_tracking_demo.avi'),\n",
    "#     path_save=None,\n",
    "    frame_height_width=videos.frame_height_width,\n",
    "    frame_rate=data.frame_rate,\n",
    "    point_sizes=3,\n",
    "    points_colors=(0,255,255),\n",
    "#     points_colors=[(cmap((factor_magnitudes / factor_magnitudes.max(0)))*255)[:,:,:3].astype(int).transpose(1,0,2)[0]],\n",
    "    alpha=0.6,\n",
    ")\n",
    "\n",
    "fr.visualization.play_video_with_points(\n",
    "    bufferedVideoReader=videos[idx_video_to_use],\n",
    "    frameVisualizer=frame_visualizer,\n",
    "    points=list(pt_data['points_tracked'].values())[0],\n",
    "    idx_frames=idx_frames_to_use,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 957.188,
   "position": {
    "height": "40px",
    "left": "1725.67px",
    "right": "20px",
    "top": "124.914px",
    "width": "628.438px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
